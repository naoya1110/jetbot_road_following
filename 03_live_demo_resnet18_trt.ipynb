{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JKwvuMxsFghR"
   },
   "source": [
    "# Road Following by Classification - Live Demo with TensorRT\n",
    "\n",
    "In this notebook, we will try to make the JetBot to follow the desired road by using the trained model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G9Ae369MGcZP"
   },
   "source": [
    "## TensorRTモデルの作成\n",
    "First we define a DNN model. This needs to be identical to what used for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torch2trt import torch2trt\n",
    "\n",
    "model = torchvision.models.resnet18(pretrained=False)\n",
    "model.fc = torch.nn.Linear(512, 3)\n",
    "model = model.cuda().eval().half()\n",
    "\n",
    "model.load_state_dict(torch.load('best_model_resnet18.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch2trt import torch2trt\n",
    "\n",
    "data = torch.zeros((1, 3, 224, 224)).cuda().half()\n",
    "model_trt = torch2trt(model, [data], fp16_mode=True)\n",
    "\n",
    "torch.save(model_trt.state_dict(), 'best_model_resnet18_trt.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zjJCDzvGFghX"
   },
   "source": [
    "## TensorRTモデルのロード\n",
    "Next we need to upload the `best_model.pth` in the file browser.\n",
    "\n",
    "Then load the parameters on the model from the `best_model.pth`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torch2trt import TRTModule\n",
    "\n",
    "model_trt = TRTModule()\n",
    "model_trt.load_state_dict(torch.load('best_model_resnet18_trt.pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BRGQRpcBFghY"
   },
   "source": [
    "## Preprocessing Function\n",
    "\n",
    "Now we create a function for preprocessing image data taken by the camera. This is very similar to what we have done in the collision avoidance example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "#mean = 255.0 * np.array([0.485, 0.456, 0.406])\n",
    "#stdev = 255.0 * np.array([0.229, 0.224, 0.225])\n",
    "\n",
    "#normalize = torchvision.transforms.Normalize(mean, stdev)\n",
    "\n",
    "def preprocess(camera_value):\n",
    "    global device, normalize\n",
    "    x = camera_value\n",
    "    x = cv2.cvtColor(x, cv2.COLOR_BGR2RGB)\n",
    "    x = x.transpose((2, 0, 1))\n",
    "    x = torch.from_numpy(x).float()\n",
    "    # x = normalize(x)\n",
    "    x = x.cuda().half()\n",
    "    x = x[None, ...]\n",
    "    x = x/255.\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1k2V5NKkHkAm"
   },
   "source": [
    "## Camera Instance\n",
    "Now we create a camera instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "EdgXKPloFgha"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02f28dcd3b9a41dda1c82cda36fb2b71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Image(value=b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x00\\x00\\x01\\x00\\x01\\x00\\x00\\xff\\xdb\\x00C\\x00\\x02\\x01\\x0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import traitlets\n",
    "from IPython.display import display\n",
    "import ipywidgets.widgets as widgets\n",
    "from jetbot import Camera, bgr8_to_jpeg\n",
    "\n",
    "image = widgets.Image(format='jpeg', width=300, height=300)\n",
    "camera = Camera.instance(width=224, height=224)\n",
    "camera_link = traitlets.dlink((camera, 'value'), (image, 'value'), transform=bgr8_to_jpeg)\n",
    "display(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference\n",
    "Let's try to make an inference. This process takes for a while for the first time because it needs to load a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    x = camera.value\n",
    "    x = preprocess(x)\n",
    "    y = model_trt(x)\n",
    "    y = F.softmax(y, dim=1)\n",
    "    y_idx = torch.argmax(y, dim=1).item()\n",
    "    print(y_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "StFLmt8uFghc"
   },
   "source": [
    "## Robot Instance\n",
    "Create the robot instance to drive the motors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "bCsq9LspFghc"
   },
   "outputs": [],
   "source": [
    "from jetbot import Robot\n",
    "\n",
    "robot = Robot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YNsLPGsMFghc"
   },
   "source": [
    "## Define Actions\n",
    "Create functions of move forward, turn left and turn right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "b8yMHZ0VFghc"
   },
   "outputs": [],
   "source": [
    "def move_forward():\n",
    "    robot.set_motors(0.3, 0.3)\n",
    "\n",
    "def turn_left():\n",
    "    robot.set_motors(0.1, 0.25)\n",
    "\n",
    "def turn_right():\n",
    "    robot.set_motors(0.25, 0.1)\n",
    "    \n",
    "actions_dict = {0:\"Go Forward\", 1:\"Turn Left\", 2:\"Turn Right\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uILzSjgmKEXg"
   },
   "source": [
    "## Run JetBot\n",
    "Run JetBot with a loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "9kLBthwAFghc",
    "outputId": "863df5c6-2f49-42ab-e48a-bcba8f0ba838"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02f28dcd3b9a41dda1c82cda36fb2b71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Image(value=b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x00\\x00\\x01\\x00\\x01\\x00\\x00\\xff\\xdb\\x00C\\x00\\x02\\x01\\x0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step:1000/1000   Action:Turn Right   FPS:63.6"
     ]
    }
   ],
   "source": [
    "import time\n",
    "t0 = time.time()\n",
    "\n",
    "display(image)\n",
    "steps = 1000\n",
    "\n",
    "softmax = torch.nn.Softmax(dim=1)\n",
    "\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "\n",
    "    for i in range(steps):\n",
    "        x = camera.value\n",
    "        x = preprocess(x)\n",
    "        y = model_trt(x)\n",
    "        y = softmax(y)\n",
    "        y_idx = torch.argmax(y, dim=1).item()\n",
    "\n",
    "        if y_idx == 0: move_forward()\n",
    "        elif y_idx == 1: turn_left()\n",
    "        elif y_idx == 2: turn_right()\n",
    "        \n",
    "        now = time.time()\n",
    "        dt = now-t0\n",
    "        t0 = now\n",
    "        FPS = 1/dt\n",
    "        \n",
    "        print(f\"\\rStep:{i+1}/{steps}   Action:{actions_dict[y_idx]}   FPS:{FPS:.1f}\", end=\"\")\n",
    "\n",
    "robot.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tHUWbWd3I6c2"
   },
   "source": [
    "If you are done, stop the robot and the camera."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "H6HPA2wWFghc"
   },
   "outputs": [],
   "source": [
    "robot.stop()\n",
    "camera.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "live_demo.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
